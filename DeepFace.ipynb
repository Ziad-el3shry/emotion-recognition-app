{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b967ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "\n",
    "st.set_page_config(page_title=\"Emotion Analyzer\", layout=\"centered\")\n",
    "\n",
    "# Emotion Analysis Function\n",
    "def analyze_emotion(image):\n",
    "    try:\n",
    "        result = DeepFace.analyze(image, actions=['emotion'], enforce_detection=False)\n",
    "        return result[0]['emotion']\n",
    "    except Exception as e:\n",
    "        st.error(f\"Analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Image Upload Handler\n",
    "def handle_image():\n",
    "    st.subheader(\"Upload an Image\")\n",
    "    uploaded_file = st.file_uploader(\"Choose an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    if uploaded_file:\n",
    "        image = Image.open(uploaded_file)\n",
    "        img_array = np.array(image)\n",
    "\n",
    "        st.image(image, caption='Uploaded Image', use_container_width=True)\n",
    "\n",
    "        emotion_scores = analyze_emotion(img_array)\n",
    "        if emotion_scores:\n",
    "            detected_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "            st.success(f\"Detected Emotion: **{detected_emotion}**\")\n",
    "            st.bar_chart(emotion_scores)\n",
    "        else:\n",
    "            st.error(\"Failed to detect emotion from the image.\")\n",
    "\n",
    "# Video Upload Handler\n",
    "def handle_video():\n",
    "    st.subheader(\"Upload a Video\")\n",
    "    uploaded_file = st.file_uploader(\"Choose a video\", type=[\"mp4\", \"avi\", \"mov\"])\n",
    "    if uploaded_file:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_file:\n",
    "            temp_file.write(uploaded_file.read())\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        cap = cv2.VideoCapture(temp_file_path)\n",
    "        if not cap.isOpened():\n",
    "            st.error(\"Error opening video file.\")\n",
    "            return\n",
    "\n",
    "        stframe = st.empty()\n",
    "        st.info(\"Processing video... This may take a while.\")\n",
    "        frame_rate = 30\n",
    "        frame_count = 0\n",
    "        detected = []\n",
    "\n",
    "        progress = st.progress(0)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            if frame_count % frame_rate == 0:\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                emotion_scores = analyze_emotion(rgb_frame)\n",
    "\n",
    "                if emotion_scores:\n",
    "                    detected_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "                    cv2.putText(rgb_frame, detected_emotion, (30, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    detected.append(detected_emotion)\n",
    "                else:\n",
    "                    cv2.putText(rgb_frame, \"Unknown\", (30, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "                stframe.image(rgb_frame, channels=\"RGB\", use_column_width=True)\n",
    "                progress.progress(min(frame_count / total_frames, 1.0))\n",
    "\n",
    "        cap.release()\n",
    "        if detected:\n",
    "            st.success(f\"Most Frequent Emotion: **{max(set(detected), key=detected.count)}**\")\n",
    "        else:\n",
    "            st.warning(\"No emotions detected from the video.\")\n",
    "\n",
    "# Main UI\n",
    "def main():\n",
    "    st.title(\"ðŸ§  Emotion Recognition App\")\n",
    "    st.markdown(\"Analyze emotions from images or videos using deep learning.\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"Choose Input Type\")\n",
    "        choice = st.radio(\"Input Source\", [\"Image\", \"Video\"])\n",
    "\n",
    "    if choice == \"Image\":\n",
    "        handle_image()\n",
    "    elif choice == \"Video\":\n",
    "        handle_video()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
